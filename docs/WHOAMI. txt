Orchestration of Robocorp Work Items with GitHub Actions

Overview

Robocorp’s fetch-repos-bot project is a reference implementation of a highly scalable producer–consumer pipeline for RPA (Robotic Process Automation). It was created to automate the fetching and processing of GitHub repositories at scale without relying on Robocorp’s paid Control Room service. Instead, it leverages free GitHub Actions as the orchestration engine. This design demonstrates “advanced orchestration, sharding, and parallelism beyond what is available in official Robocorp examples” by using a matrix-sharded producer–consumer architecture. In other words, the bot splits its workload into many small work items that can be processed in parallel by multiple runners, achieving a level of concurrency and throughput ideal for large-scale automation.

**Key highlights of this approach include:**

True producer–consumer separation: The pipeline cleanly separates the Producer task (which fetches data and prepares work items) from the Consumer tasks (which process those items). Handoff between producer and consumers is done via artifacts (files), mimicking a queue of work items. This ensures each stage is isolated and can scale independently.

Matrix sharding for parallelism: The bot dynamically divides the work into shards and uses a GitHub Actions matrix strategy to spawn parallel consumer jobs. Each consumer job picks up a unique shard of work items, maximizing throughput by running in parallel. This effectively replaces the fan-out behavior of Robocorp’s Control Room with GitHub’s workflow engine.

RCC-managed environments: All robot runs use Robocorp’s RCC (Robocorp Cloud Container) to manage the Python environment. This guarantees reproducibility and isolation in both local and CI contexts. The custom GitHub runner Docker image (for self-hosted execution) is only rebuilt when environment definitions change (such as conda.yaml or robot.yaml), so routine code changes don’t trigger full environment rebuilds. This makes the automation stable and efficient in CI.

Open and extensible orchestration: The entire orchestration logic is implemented with code (Python scripts and GitHub workflows) that is open and customizable. Unlike a black-box cloud service, this approach lets automation engineers tweak how sharding, environment setup, and parallel execution work. It provides a blueprint for building large-scale, cloud-native RPA pipelines on free infrastructure.


Robocorp Work Items and Orchestration Background

Work items in Robocorp are the mechanism for passing data between steps of an automated process. In the official Robocorp Control Room (the proprietary orchestrator), work items act as units of data that flow from one robot task to the next. *“Work items are used in Robocorp Control Room for managing data that go through multiple steps and tasks inside a process. Each step of a process receives input work items from the previous step, and creates output work items for the next step.”* In other words, a producer task can output a collection of work items (each containing some payload, e.g. a repository info), and the control room will queue these up for one or more consumer tasks to pick up. This allows a natural producer–consumer pattern: one robot generates jobs, and subsequent robots handle those jobs, potentially in parallel.

What makes Robocorp’s approach unique is this built-in concept of work item queues, which is not common in all automation frameworks. It enables robust error handling (tracking which item failed), re-execution of only failed items, and scaling out processing by adding more workers for multiple items. Typically, using these features requires the Robocorp Control Room (a paid cloud service) to manage the queues and trigger tasks. The creator of fetch-repos-bot wanted to retain the benefits of work items – splitting a large job into independent pieces and processing them concurrently – without using the paid Control Room. This meant replicating the work item orchestration logic using open tools.

Producer–Consumer Architecture in fetch-repos-bot

The fetch-repos-bot project implements a classic two-step pipeline using Robocorp’s robot framework: a Producer step and a Consumer step, with clear separation and artifact-based handoff. Here’s how it works in practice:

1. Producer Step: The producer robot fetches repository data from a specified GitHub organization (using GitHub’s API). It collates this data into a list of “work items” – for example, each work item could be one repository with its metadata. The producer then outputs these items as a JSON file artifact. In Robocorp terms, it “generates work items” for the next step. In this project, the output is saved to output/producer-to-consumer/work-items.json (a local file acting as the queue). The producer job then uploads this file as a build artifact in the GitHub Actions workflow for downstream use.


2. Sharding and Matrix Generation: After the producer finishes, a Python helper script generate_shards_and_matrix.py runs to split the work items into shards for parallel processing. This script reads the list of work items and divides it into N shards, where N is either a user-specified maximum or the number of items (whichever is smaller). Each shard is written out as its own JSON file (e.g., work-items-shard-0.json, ...-shard-1.json, etc.). The script also produces a matrix configuration file (matrix-output.json) that tells GitHub Actions how many shards were created and includes an array of shard IDs. This dynamic generation of the matrix is key – it allows the workflow to fan-out exactly one consumer job per shard in the next step. The workflow uses the matrix JSON to configure parallel jobs, meaning the number of parallel consumer jobs adapts to the data size on the fly.


3. Consumer Steps (Parallel): GitHub Actions then launches the Consumer jobs as a matrix, with each job receiving a different shard_id from the matrix include list. The workflow ensures each consumer job first downloads the producer’s artifact (containing all shard files) so it has access to its assigned shard’s data. For each consumer job, an environment is configured so that the Robocorp work item library will only process the specific shard file assigned to that runner. This is achieved by setting special environment variables for RCC’s work item “FileAdapter” mode: the job sets RC_WORKITEM_ADAPTER=FileAdapter, along with RC_WORKITEM_INPUT_PATH="output/shards/work-items-shard-X.json" (where X is the shard ID) and a corresponding output path. According to Robocorp’s docs, setting these variables tells the robot to treat a local JSON file as the input queue instead of expecting Control Room to provide work items, and to write results to an output JSON file. Each consumer robot (running via RCC) picks up the work items from its shard file and processes them – for example, cloning the repository and extracting whatever data or performing whatever analysis the task is designed for. After processing all items in its shard, the consumer robot writes its output (if any) to the designated output JSON (and any other artifacts). The GitHub Actions job then uploads the results as an artifact named for that shard.


4. Result Collection: With all consumer jobs running in parallel, the overall workflow completes much faster than a single-threaded process would. Each consumer produces an output artifact (or they could all write to a common storage if needed). In this reference implementation, the artifacts from each shard are kept separate (e.g., one per shard), but one could imagine a follow-up step to merge results if necessary. The key is that the orchestration ensured each shard was handled exactly once, and all shards were processed concurrently. If any shard failed, that failure would be isolated to its job, and one could re-run just that consumer job if needed (similar to re-running a failed work item in Control Room, but achieved by re-running the GH Action for that shard).



This architecture effectively emulates a Robocorp Control Room “Process” using GitHub Actions. In Control Room, you might have a process with two steps: the first creates N work items and the second step automatically spawns N parallel runs to handle them. Here, the combination of artifact handoff and matrix jobs achieves the same outcome. The result is a free orchestration solution: GitHub’s runners and artifacts service handle the distribution of work at no cost (beyond any infrastructure you choose to self-host), avoiding the need for Robocorp’s paid cloud orchestration.

Dynamic Matrix Sharding in GitHub Actions

One notable innovation in this project is how it dynamically generates the matrix for parallel jobs. GitHub Actions normally requires you to define the job matrix (the set of parallel jobs and their parameters) in advance in the workflow YAML. However, in fetch-repos-bot, the number of shards (and thus consumer jobs) is determined at runtime by the amount of work. The solution uses the producer job to calculate the matrix, then passes it to the next job. Specifically, after the producer creates the matrix-output.json file, the workflow reads that file and sets an output value that encapsulates the JSON matrix definition. The consumer job is configured with:

strategy:
  matrix: ${{ fromJSON(needs.producer.outputs.matrix).matrix }}

This clever trick injects the dynamically created matrix (the list of shard IDs) into the workflow, causing GitHub Actions to spawn one consumer job per shard. In essence, the workflow “writes” its own parallelization plan on the fly. Each job in the matrix knows its shard_id via the matrix, and the Python helper shard_loader.py (invoked in each job) can use that to prepare the correct input file for processing. By designing the system this way, the developer achieved massive horizontal scalability – whether there are 5 repositories or 5000, the workflow can scale out to handle them by just increasing the max_workers input. This is highlighted as *“matrix sharding for massive parallelism — each consumer job processes a unique shard, maximizing throughput.”*.

Another benefit of this approach is that it keeps each execution unit (each runner container) focused on a subset of work, which can improve reliability. For example, if one repository’s processing crashes, it won’t stop the others – that shard’s job can fail independently while others succeed. This granular parallelism is similar to worker threads in a queue system or lambda functions in serverless architectures, but it’s implemented purely with GitHub’s CI features and Robocorp’s ability to handle work item files.

RCC Environment Isolation in CI/CD

A crucial aspect of making this orchestration work smoothly is environment management. Robocorp provides RCC, a command-line tool that creates and runs robots in self-contained Python environments (using Conda under the hood). The fetch-repos-bot uses RCC extensively to ensure that whether it runs locally on a developer’s machine or on a GitHub runner, the robot’s dependencies and Python versions are consistent. The project’s robot.yaml and conda.yaml define the robot’s required environment (libraries, Python version, etc.), and RCC takes care of setting that up in an isolated manner.

In the GitHub Actions workflows, the first steps ensure RCC is installed on the runner, then run rcc run -t producer or rcc run -t consumer to execute the respective tasks. Because RCC is managing the environment, there is no need to manually install Python packages in the CI job – RCC will create or reuse the environment defined by conda.yaml. This yields reproducibility: the same robot environment is used everywhere, eliminating “it works on my machine” problems. The README emphasizes that *“all environments are fully managed and isolated by RCC… for both local and CI runs.”*.

Another advantage in CI is efficiency: RCC can cache environments, and the project even provides a Docker image for the runner with the environment pre-installed. That means that if you run the pipeline repeatedly, you aren’t re-installing dependencies each time – the environment remains stable unless you change something fundamental like the conda or robot configuration. In fact, the custom runner Docker image (which bundles Python and the needed libraries) is only rebuilt when conda.yaml, robot.yaml, or the Dockerfile changes; *“for all other code or workflow changes, the environment remains stable.”* This dramatically speeds up iterative development and CI runs, as most changes (like adjusting the logic in tasks.py) don’t trigger a full environment rebuild. Instead, RCC reuses the existing environment snapshot. This approach is in contrast to typical CI pipelines where a fresh pip install might happen on every run – here the environment is handled more like an artifact that only updates when necessary.

Security and isolation are also improved with RCC. Each robot run happens in a sandboxed environment (no global Python packages needed on the runner, minimal risk of dependency conflicts on the host). This is especially important when using self-hosted runners, where you may not want the base system polluted by dozens of RPA library dependencies. RCC gives a clean separation between the runner’s OS and the robot’s libs.

Scaling Out with Self-Hosted Runners (ARC and Kaniko)

While GitHub’s own hosted runners can be used (and the project includes a workflow for GitHub-hosted execution), fetch-repos-bot also supports running on self-hosted runners in a scalable way. The repository includes a repos/ directory with a Dockerfile and Kubernetes helm chart values intended for the **GitHub Actions Runner Controller (ARC)**. ARC is a solution that manages a pool of self-hosted GitHub runners on a Kubernetes cluster, allowing them to scale up automatically in response to jobs. By building a custom runner image that has RCC and all robot dependencies pre-installed, the project ensures that when a new runner pod starts, it can immediately run the Robocorp tasks without spending time on setup.

To facilitate building this custom image in CI, the repo provides a Kaniko workflow. Kaniko is a tool for building container images inside a Kubernetes or container environment where Docker might not be available. The build-kaniko-docker.yaml workflow *“builds and pushes Docker images using Kaniko for environments where Docker-in-Docker is not available.”* This is useful because, for security reasons, you might not run Docker daemon on a cloud CI runner. With Kaniko, the CI can still produce the updated runner image (for example, after adding a new library to conda.yaml) and push it to a registry. Then ARC can pull this image for new runner instances. The repository’s workflows are organized to handle both hosted and self-hosted scenarios: there’s a matrix workflow for hosted runners, and a matrix workflow tailored for self-hosted runners (which likely uses labels or runs-on that correspond to the ARC-managed runners).

By using ARC with a scalable set of runner containers, this solution can achieve truly massive parallelism without being limited by GitHub’s hosted runner concurrency. Essentially, the Kubernetes cluster can autoscale to, say, 50 or 100 pods if needed, each picking up a shard of work. This is analogous to Robocorp’s cloud scaling out robot workers, but here it’s accomplished with open-source infrastructure. The Actions Runner Controller handles the lifecycle of runners (creating pods when jobs are queued and removing them when done), so the automation engineer doesn’t have to manually manage machines or containers. Combining this with the dynamic matrix from earlier, the whole pipeline can scale out to whatever degree is required by the workload.

It’s worth noting that setting up one’s own orchestration (even with GitHub Actions) requires managing more pieces (Kubernetes, container images, etc.), but the payoff is avoiding vendor lock-in and fees. This project shows that with some engineering effort, one can replace a proprietary orchestration service with standard DevOps tools and cloud resources.

Avoiding a Custom Control Room Solution

The creator of fetch-repos-bot deliberately chose GitHub Actions as the control mechanism instead of building a custom orchestrator or Control Room clone. Earlier attempts included exploring the Sema4.ai Action Server (an open-source project that can run Robocorp bots and was derived from Robocorp’s own Action Server for AI activities). However, maintaining a full custom “control room” (with scheduling, a UI for monitoring, etc.) proved complex – for example, the Sema4.ai server’s frontend components were bundled in a way that made modifications difficult. Rather than spend time reinventing those wheels, the developer leveraged GitHub Actions which already provides: a UI for viewing logs and statuses, a scheduling mechanism (via triggers or cron), permission management, and a way to store secrets and artifacts.

By using GitHub Actions, the solution benefits from an existing ecosystem:

Scheduling: Workflows can be triggered on a schedule (cron) or on-demand (workflow_dispatch), similar to scheduling runs in Control Room.

Logging and Monitoring: Each job’s logs are available in the GitHub Actions interface, and job statuses (success/failure) are clearly indicated, fulfilling the role of run monitoring.

Artifacts & Data Handling: The built-in artifact storage substitutes for the Control Room’s work item storage. As we saw, artifacts carry the work item JSONs between jobs.

Concurrency Control: GitHub Actions can limit or allow concurrent runs, and ARC can ensure resources scale accordingly. This is akin to controlling how many robots run in parallel.

Cost: For moderate workloads, GitHub’s free tier may suffice, and beyond that, self-hosted runners can be used at the cost of the compute infrastructure (which often is cheaper or more flexible than per-minute cloud RPA charges).


In short, the project demonstrates a path to free orchestration of RPA processes: it sidesteps the need for Robocorp’s cloud by using GitHub’s CI/CD as the orchestrator. This is quite unique in the RPA world – most RPA platforms either lock you into their orchestrator or require a lot of custom code to achieve similar results. Here, by embracing infrastructure-as-code and DevOps practices, the solution achieves a high level of scalability and reliability without a proprietary control room.

Conclusion

fetch-repos-bot serves as a powerful example of how to build a large-scale, parallel RPA pipeline using open tools. It marries Robocorp’s robust automation stack (Python-based robots with work items, RCC for environment management) with GitHub Actions for orchestration. The result is a blueprint that is “designed for true scale” and *“suitable for both local and CI/CD automation.”* By dynamically sharding work and distributing it to parallel tasks, the pipeline can process workloads that would be infeasible for a single robot runtime. All of this is achieved with a transparent, code-defined approach – anyone can inspect the workflows and scripts to understand how it works, and modify it for their own needs.

In a landscape where many RPA solutions require paid cloud services for scaling up, this project stands out by delivering orchestration for free. It shows that with a bit of ingenuity, one can replace a paid control room with existing CI/CD infrastructure and still get features like work item queues, parallel execution, and failure isolation. For DevOps and automation engineers, this approach offers a compelling alternative: use familiar tools (git, YAML pipelines, Docker, Kubernetes) to run software robots at scale, and avoid getting locked into RPA-specific platforms. Ultimately, fetch-repos-bot illustrates the art of combining RPA and DevOps to achieve scalable automation on your own terms – no proprietary control room required.

Sources: The information above is drawn from the fetch-repos-bot repository documentation, its workflow and code files, and Robocorp’s official work items documentation, which together detail the design and rationale of this orchestration solution.

